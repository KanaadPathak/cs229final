
%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/



% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[journal, 10pt]{IEEEtran}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%

\usepackage[pdftex]{graphicx} % Enhanced support for graphics
% declare the path(s) where your graphic files are
\graphicspath{{../images/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpg,.png}

% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/

\usepackage{amsmath}   % AMS math­e­mat­i­cal fa­cil­i­ties
\usepackage{amsthm}    % Type­set­ting the­o­rems (AMS style)
\usepackage{amssymb}   % AMS symbol fonts
\usepackage{mathtools} % Math­e­mat­i­cal tools to use with ams­math
\usepackage{physics}   % Macros sup­port­ing the Math­e­mat­ics of Physics
\usepackage{esvect}    % Vec­tor ar­rows



% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/

\usepackage{listings} % Typeset source code listings
% \usepackage[]{mcode}


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array} % Ex­tend­ing the ar­ray and tab­u­lar en­vi­ron­ments
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/

%% Table, Bulletin and other type­set­ting
\usepackage[ampersand]{easylist} % Lists us­ing a sin­gle ac­tive char­ac­ter
% \usepackage[table,x11names,svgnames]{xcolor} % Driver-in­de­pen­dent color ex­ten­sions
\let\labelindent\relax
\usepackage{enumitem}      % Con­trol lay­out of item­ize, enu­mer­ate, de­scrip­tion
% \usepackage{multirow}      % Create tab­u­lar cells span­ning mul­ti­ple rows
% \usepackage{tabularx}      % Tab­u­lars with ad­justable-width columns
% \usepackage{tablefootnote} % Per­mit foot­notes in ta­bles


%% Sectioning and Paging
\usepackage{multicol}
% \usepackage{fancyhdr}   % Ex­ten­sive con­trol of page head­ers and foot­ers
% \usepackage{titlesec}   % Select al­ter­na­tive sec­tion ti­tles
% \usepackage{afterpage}  % Ex­e­cute com­mand af­ter the next page break
% \usepackage{parskip}    % Lay­out with zero \parindent, non-zero \parskip
% \usepackage{microtype}  % Sublim­i­nal re­fine­ments to­wards ty­po­graph­i­cal per­fec­tion


%% Language and font encodings
% \usepackage{textcomp}        % Text Com­pan­ion fonts
% \usepackage{lmodern}         % Latin Modern fonts
% \usepackage{soul}            % Hyphen­ation for let­terspac­ing, un­der­lin­ing, and more





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/

%% Figure, Listing and Reference
% \usepackage{subcaption} % Sup­port for sub-cap­tions
% \usepackage{verbatim}   % Reimplementation of and extensions to LATEX verbati





% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley and Jeff Goldberg.
% This package may be useful when used in conjunction with IEEEtran.cls'
% captionsoff option. Some IEEE journals/societies require that submissions
% have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.3.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% For subfigure.sty:
% \let\MYorigsubfigure\subfigure
% \renewcommand{\subfigure}[2][\relax]{\MYorigsubfigure[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat/subfig command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.

\usepackage{float}    % Im­proved in­ter­face for float­ing ob­jects
\usepackage{placeins} % Control float placement





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.

\usepackage[final]{pdfpages} % In­clude PDF doc­u­ments
\usepackage[colorlinks=true, allcolors=blue]{hyperref} % Ex­ten­sive sup­port for hy­per­text
\usepackage{wrapfig}





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Plant Leaf Recognition}
%
%
% author names
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
\author{Albert~Liu, albertpl@stanford.edu
        Yangming~Huang, yangming@standford}% <-this % stops a space
% \thanks{Nihit Desai from Computer Science Department, Stanford University}% <-this % stops a space

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.


% make the title area
\maketitle

%\begin{abstract}
%\end{abstract}
\section{Introduction}
Automatic plant species recognition with image processing has application in weeds identification, species discovery, plant taxonomy, natural reserve park management and so on \cite{Pedro13}.  It is considered a fine-grained image recognition problem which is hard to solve since
\begin{enumerate}
  \item The subtle differences between different species in the same class. Sometime such fine differences can be even challenging to human experts.
  \item Typically this requires large training data but it is not feasible due to the number of species (over 220000 \cite{Charles13}).
\end{enumerate}

In this report, we describe our exploration with this problem, using traditional handcrafted features and features extracted from pretrained deep convolution neural network (ConvNets). The input to our system is raw images from various datasets and the output is the label for each species.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{easilyconfused}
\end{figure}

% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

\section{Related Work}
Research on automatic leaf classification with computer vision processing has been active since 2000. Lots of hand-crafted features have been proposed, ranging from shape based, to statistical texture and margin \cite{Charles13} \cite{Pedro13} \cite{Cho2002}. Also generic computer vision object recognition/detection features, such SIFT\cite{SIFT} and HOG\cite{HOG} are studied for this problem.  Most of such manually engineered features achieve excellent accuracy on clean images taken in controlled conditions, which consist of one single well aligned leave on contrasting background, such as those images in data set \cite{SwedishLeafDataset}.

Recently, with the huge success of deep ConvNets, particularly from the winners of ILSVRC \cite{Alex2014} \cite{VGGNetReference}, \cite{GoogleNetReference} and \cite{ResNet}, researchers start to apply deep ConvNets to this problem.  In \cite{CNNOfTheShelf}, Sharif, et al. suggested that generic features can be extracted from large ConvNet and yield very good results on fine-grained classification problems even without fine-tuning the pretrained model.  \cite{EvaluationOfLeafConv} compares traditional approaches with ConvNet based approach, and discuss impacts on various conditions, including translation, rotation, scale and occlusion. However it only give results on Flavia\cite{FlaviaDataset} dataset which is relatively less challenging\ref{DataSet}. \cite{PlantIdentificationConv} proposes a VGG\cite{VGGNetReference} based architecture, and use multiple organ features. It produces above 70\% mean average precision on ImageCLEF dataset\cite{ImageCLEF2013}.


\section{DataSet}
\label{DataSet}
  \begin{tabular}{c@{}c@{}c@{}c@{}c@{}}
  {\includegraphics[height=6.0em]{swedish1}} &
  {\includegraphics[height=6.0em]{flavia1}} &
  {\includegraphics[height=6.0em]{clef1}} &
  {\includegraphics[height=6.0em]{clef2}} &
  {\includegraphics[height=6.0em]{clef3}}
  \end{tabular}

    We consider two types of dataset.
\begin{enumerate}
  \item Swedish\cite{SwedishLeafDataset} and Flavia\cite{FlaviaDataset}. Both contain clean images only, which is characterized with well aligned leaf on contrasting background, with little or no variations of luminance or color. \\

  \item ImageCLEF \cite{ImageCLEF2013}, which is collected through crowd sourced application. This is a much more noisy datasets with considerable variations on lighting conditions, viewpoints, background clutters and even occlusions. The dataset can be further split into two subset: uniform, which is taken in a more controlled environment, and natural, which is taken in a natural environment.
  \begin{center}
      \begin{tabular}{| c | c | c |}
      \hline
      Name    & Species & samples \\ \hline
      uniform & 66      & 9607 (train)        1194 (test)         \\ \hline
      natural & 57      & 2585 (train)        521  (test)       \\ \hline
      Swedish & 15      & 75        \\ \hline
      Flavia  & 33      & $\sim$ 60 \\ \hline
      \end{tabular}
  \end{center}
\end{enumerate}

\section{Approach}

\subsection{Overview}
Here is the pipeline of our system.
\begin{figure}[H]
  \center
  \includegraphics[width=1.0\linewidth]{overview}
  \caption{ Overview of the system }
  \label{fig:pipeline}
\end{figure}

\begin{enumerate}
  \item Firstly, during preprocessing, we apply CLAHE\cite{CLAHE} to reduce lighting condition variation and then resize raw images to fit the next layer. We also attempt to remove background with the following techniques in a heuristic way
  \begin{itemize}
    \item use K-means (K=2) and throw away background
    \item find the convex hull containing the largest N contours and then use GrabCut\cite{Grabcut} to segment leaf out of the background clutter.
  \end{itemize}
\item Next we extract features
\begin{itemize}
  \item ConvNets

    We take transfer learning approach, in order to make use of the power of deep ConvNets under the constrains of time and computations. Specifically, we take a couple of ConvNets that are pretrained on ImageNet for ILSVRC object classification task, remove top FC layers and then treat the rest of the ConvNet as fixed feature extractor. The CNN codes are our features. To battle overfitting issue, we augment our input images with color channel shift, translation and rotation.
  \item Traditional SIFT + Bag of Features

    Key points are densely sampled and SIFT feature descriptors are retrieved at each key point. We fix size of the codebook (K) as 1000/300 for different data sets.
\end{itemize}
\item Finally,  we train a simple classifier from the feature vectors and then predict labels for our test data.
\end{enumerate}
\subsection{ConvNets approach}
  \subsubsection{Setup}
  For ConvNets, we used Keras framework \cite{Keras} with Tensorlow backend of GPU support: NVIDIA GeForce GT 750M 2048 MB. As an alternative, we also run on CPU given the limited graphic memory of our GPU which is crucial for a deeper ConvNets architecture.

  \subsubsection{Exploration}
  We started by training ConvNets classifier from scratch, following the guidelines below:
  \begin{enumerate}
    \item Convolutional layer learning features from general to specific, giving more layers helps with the transition. Study on deep convolutional nets suggests that deeper models are preferred under a parameter budget\cite{Deep2013}.
    \item Dropout reduces overfitting \cite{Dropout} \cite{Alex2014}
    \item Use aggressive pooling to reduce the dimensionality.
  \end{enumerate}
  We designed our ConvNets with the architecture below:
  \begin{figure}[H]
    \center
    \includegraphics[scale=0.4]{cnn_arch}
    \caption{Customized ConvNets Classifier Architecture}
    \label{cnn_arch}
  \end{figure}
  Using cross entropy to measure loss, we got the learning curve:
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{cnn_loss}
    \caption{Loss curve of ConvNets Classifier}
    \label{cnn_loss}
  \end{figure}
  The loss of swedish dataset \cite{SwedishLeafDataset} keeps decreasing as expected. But for flavia dataset \cite{FlaviaDataset}, the training loss is keep decreasing, but at some point, validation loss stop changing. For imageclef uniform dataset \cite{ImageCLEF2013}, it actually stopped learning and the loss goes back up and stay there.

  The accuracy is consistent from the loss, where accuracy of swedish converges at $92\%$, with a test accaracy of $90.46\%$, but it doesn't work well with other datasets especially imageclef dataset.
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{cnn_acc}
    \caption{\label{fig:cnn_acc} Accuracy curve of ConvNets Classifier}
    \label{cnn_acc}
  \end{figure}

  Compare the datasets, intuitively, there are a few things to consider:
  \begin{itemize}
    \item The lighting variation and background clutter. Swedish has very clean background, where for ImageCLEF uniform image quality varies in a large range.
    \item More species with less samples per classes.
  \end{itemize}

  Some of our data set such as ImageCLEF2013 is even more challenging in term of clutter and occlusion, which we didn't cover in the customized ConvNets classifier experiment.

  A deeper ConvNets with more aggressive filters is needed to extract the features more efficiently and to deal with the noise and variation of datasets. But given the constrains of time and computations, we can't afford to train a deeper network with much more parameters.

  As a common practice, we switched to use pre-trained weights of proven ConvNets architecture. Since the output is different for our specific problem, we cannot apply the architecture of the pre-trained weights directly. We used Transfer Learning which will be discussed in next subsection.

  \subsubsection{Transfer Learning}
  The two well-known options for Transfer Learning are:
  \begin{enumerate}
    \item ConvNet as fixed feature extractor, and then classify with other Classifiers such as Logistic Regression/Softmax or SVM
    \item Fine-tuning the ConvNet. For a $N$ level structure, train the last $H$ levels with the $N-H$ lower levels freeze (the higher the level, the less overfitting to the target-datasets) \cite{CS231N}.
  \end{enumerate}

  There are several options of architecture trained against the well-known ImageNet whose weights are available with keras framework.
  \begin{itemize}
    \item VGGNet, runner-up in ILSVRC 2014 (GoogLeNet is the winner of that year).
    \item ResNet, winner of ILSVRC 2015. The available weights are for ResNet50 with 50 layers (including Fully Connected layers) in total.
  \end{itemize}

  The ideal situation to choose an architecture of pre-trained weights is that it has been trained against original datasets that is similar to the target datasets. ImageNet has $14,197,122$ images, $21841$ synsets. Among them, there are $70$ synsets that is related to leaves. Given the large number of syncsets, the weights trained against ImageNet is generalized enough that there will be relatively less bias. The weights are trained for the object image classification task of ImageNet, which is aligned with our task except that ours are more fine-grained leaf classification.

  Further more, Jason et al. found that even features transferred from distant tasks are better than random weights \cite{Jason2014}. Also note that ConvNet features are more generic in early layers and more original-dataset-specific in later layers\cite{CS231N}.

  After comparing preliminary results, we choose ResNet50 since ResNet50 gives better results and less overfitting. We believe this can be attributed to the fact that ResNet50 is deeper, but still having lower complexity\cite{ResNet}. It also generates lower dimension feature vector, which is likely due to the use of a more aggressive Average Pooling with a pool size of 7x7. This saves us from the effort to seek for reduction of dimensionality.

  % [explain why species with less sample will reduce the performance. Explain why sample per classes makes it harder to predict Learning Theory] [TODO]

  The ResNet is famous for it's deep layers\cite{ResNet}, in our case, 50 layers, with 49 Conv layers and one FC layer on top. Except for the first Conv layer, the rest 48 composes 16 ``residual'' blocks in 4 stages. The block within each stage has similar architecture, i.e. same input \& output shape. \par

  The possible approaches as forementioned, are either get the bottleneck features which is called CNN codes in the terms of transfer Learning, or freeze all the layers except for the last Residual Block and train with the target datasets. See Figure \ref{fig:tl_resnet}.

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.23]{TL_ResNet}
    \caption{ Illustration of Transfer Learning with ResNet }
    \label{fig:tl_resnet}
  \end{figure}
  Again, with the constrains of the time and computations, we choose to just extract the CNN codes.

  We download the weights and load it into the preset model. And drop off the layer after the last Convolutional/Residual block. Feed the network with augmented data, the CNN codes (feature vector) we get from ResNet50 is 2048 dimensions.

  With the help of visualization, intuitively we can see that each filter is seizing different features. The output from later layers is more abstract and global than the earlier layers. We can also see that some of the filters are completely dark, means for that particular filter, it doesn't response to certain feature of the image after rectification.

  \begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.3\linewidth}
      \centering
      \includegraphics[width=0.9\textwidth]{33214-conv1.jpg}
      \vspace{4ex}
    \end{minipage}%
    \begin{minipage}[b]{0.3\linewidth}
      \centering
      \includegraphics[width=0.9\textwidth]{33214-a1.jpg}
      \vspace{4ex}
    \end{minipage}
    \caption{Visualization of partial outputs for each stage of ResNet50. Left to right, top to bottom, they are from first convolutional layer, the activation layer right after respectively. Note that the 4 outputs of Conv layer are corresponding to the 4 outputs of the activation layer}
    \label{fig:visual_output}
  \end{figure}

  Thus, we can assume that the pre-trained weight is applicable to our problem, at least the early layers do generalize well to our feature space.

  \subsection{Discussion}
    How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios:
    \cite{CS231N}

    \begin{enumerate}
      \item similarity (transfer learning is good)
      \item size for each class(fine tuning is not a good idea)
    \end{enumerate}

    \subsection{SIFT + Bag of Features (BoF) }
   Due to the simplicity and performance, this well established approach was taken at first.  We prototyped the system with OpenCV libraries.  Here is the illustration of the system \ref{fig:bofsystemdesign}.
 Key points are densely sampled with a step size of 20 from the grays caned copies of the training set. Then we extract SIFT descriptor for each key point, which are clustered to build visual words via K-Means. Several codebook size (500, 1000, 3000) are used and we pick the one with the best validation results for each data set. To reduce computation complexity, we randomly select 100 training images to build the codebook.  Finally, each train and test sample is represented with histograms of visual words, i.e. term vectors, and this is used as feature vectors for classification.
\begin{figure}[H]
  \centering
  \includegraphics[width=1.00\linewidth]{bof}
  \caption{ Bag of Features }
  \label{fig:bofsystemdesign}
\end{figure}

\section{Experimental Results}
We use prediction rank-1 identification (i.e. accuracy) as our performance metric, which is defined as $Accurary = \frac{N_c}{N_t} \times 100 \%$, where $N_c$ represents the number of correct match and $N_t$ is the total number of test samples.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.00\linewidth]{best_cm_uniform}
  \caption{ Confusion Matrix }
  \label{fig:best_cm_uniform}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1.00\linewidth]{test_accuracy_with_customized_cnn}
  \caption{ test Accuracy }
  \label{fig:test_accuracy}
\end{figure}

\begin{tabular}{c@{}c@{}c@{}c@{}}
Dataset       & SIFT\_BOF & TL\_ResNet50  & Customized\_CNN \\
\hline
Swedish       & $97.70\%$ & $99.33\%$     & $94.44\%$ \\
Flavia        & $96.84\%$ & $97.81\%$     & $49.80\%$ \\
CLEF\_Uniform & $47.40\%$ & $71.11\%$     & $0.50\%$  \\
CLEF\_Natural & $10.75\%$ & $47.22\%$     &
\end{tabular}

\section{Discussions}
\begin{enumerate}
  \item As expected, CNN codes off the shelf yields similar or better accuracy, compared to SIFT+BoF. Particularly traditional method suffers on noisy datasets.  We use t-SNE \cite{tSNE} to visualize the feature vectors and it is clear that feature tend to spread further in Image CLEF Natural dataset. And the combination of Natural data set + traditional method gives the most sprawling representation in feature space.
  \begin{tabular}{c@{}c@{}}
  {\includegraphics[width=0.50\linewidth]{tsne_uniform_bof}}  &
  {\includegraphics[width=0.50\linewidth]{tsne_uniform_tl}} \\
  {\includegraphics[width=0.50\linewidth]{tsne_natural_bof}}  &
  {\includegraphics[width=0.50\linewidth]{tsne_natural_tl}} \\
  \end{tabular}
  {\tiny Top left: Uniform BoF. Top right: Uniform Transfer Learning. Bottom left: Natural BoF. Bottom right: Natural Transfer Learning.}

  \begin{table}
      \scriptsize
    \begin{tabular}{c@{}c@{}c@{} l }
  {\includegraphics[height=0.6in]{20326}}  &
  {\includegraphics[height=0.75in]{20326_pool5}}  &
  {\includegraphics[height=0.75in]{20326_pool5_119_deconv}} &
  \begin{tabular}[b]{@{}c@{}}
      shower curtain(0.21) \\
      window shade (0.19)  \\
      cup (0.11)\\
      theater curtain (0.07) \\
      tub (0.07)
    \end{tabular}
  \\
  {\includegraphics[height=0.75in]{20326_clean}}  &
  {\includegraphics[height=0.75in]{20326_clean_pool5}}  &
  {\includegraphics[height=0.75in]{20326_clean_pool5_485_deconv}} &
  \begin{tabular}[b]{@{}c@{}}
        pot (0.57) \\
        strawberry (0.19)\\
        vase (0.04)\\
        buckey (0.03) \\
        hip (0.02)
    \end{tabular}
  \\
  {\includegraphics[height=0.75in]{6139}}  &
  {\includegraphics[height=0.75in]{6139_pool5}}  &
  {\includegraphics[height=0.75in]{6139_pool5_697_deconv}}  &
  \begin{tabular}[b]{@{}c@{}}
  lampshade(0.17) \\
  tick(0.10) \\
  gong(0.08) \\
  orange(0.07) \\
  bonnet (0.04)
    \end{tabular}
  \\
  {\includegraphics[height=0.75in]{2469}}  &
  {\includegraphics[height=0.75in]{2469_pool5}}  &
  {\includegraphics[height=0.75in]{2469_pool5_1227_deconv}}  &
  \begin{tabular}[b]{@{}c@{}}
  spider web (0.19) \\
  orange (0.11) \\
  lemon (0.10) \\
  lycaenid (0.07) \\
  parachute (0.07)
    \end{tabular}
  \\
  {\includegraphics[height=0.75in]{5078}}  &
  {\includegraphics[height=0.75in]{5078_pool5}}  &
  {\includegraphics[height=0.75in]{5078_pool5_851_deconv}}  &
  \begin{tabular}[b]{@{}c@{}}
  vase (0.14) \\
  spotlight (0.11) \\
  daisy (0.05) \\
  goblet (0.04) \\
  candle (0.04)
    \end{tabular}
  \\
  {\includegraphics[height=0.75in]{5812}}  &
  {\includegraphics[height=0.75in]{5812_pool5}}  &
  {\includegraphics[height=0.75in]{5812_pool5_486_deconv}}  &
  \begin{tabular}[b]{@{}c@{}}
  lacewing(0.21) \\
  pot (0.10) \\
  vase (0.05) \\
  hair slide (0.05) \\
  dragonfly(0.05)
    \end{tabular}
  \\
  {\includegraphics[height=0.75in]{675}}  &
  {\includegraphics[height=0.75in]{675_pool5}}  &
  {\includegraphics[height=0.75in]{675_pool5_1501_deconv}} &
  \begin{tabular}[b]{@{}c@{}}
  lacewing (0.42) \\
  nematode (0.28) \\
  ear (0.05) \\
  damselfly (0.02) \\
  dragonfly (0.02)
    \end{tabular}
  \\
  {\includegraphics[height=0.75in]{4879}}  &
  {\includegraphics[height=0.75in]{4879_pool5}}  &
  {\includegraphics[height=0.75in]{4879_pool5_110_deconv}} &
  \begin{tabular}[b]{@{}c@{}}
  nematode (0.95) \\
  dragonfly (0.02)  \\
  lacewing (0.01)
    \end{tabular}
  \\
  \end{tabular}
  \caption{Feature map and deconv for pool5 layer}
  \label{tbl:vizpool5}
\end{table}
\item To understand what stimulate neurons and how variations affect such stimulus, we visualize the feature map and deconv\cite{deconv} of pool5 layer of ResNet for several pairs of data, using the visualization toolbox \cite{DeepVizTool}, in table \ref{tbl:vizpool5}.  Except for the last pair, all pairs come from the same species. And we can tell roughly that background clutter and color give quite different CNN codes.
      \begin{itemize}
        \item [--] Background clutter yields the biggest confusions for ConvNet. None of the top 5 ILSVRC labels match and the stimulated neurons are quite different.
        \item [--] Color and scale seem to activate slightly different sets of neurons.
      \end{itemize}

\item As an exercise for error analysis, we manually inspect the results of two preprocessing methods and pick the best out of these two for Image CLEF Uniform data set.

    \begin{tabular}{ c c c }
    \hline\hline
    No preprocessing & K-means & Hand-picked \\
    63.23\% &  58.79\% & 71.11\% \\
    \hline
  \end{tabular}

This clearly tells that the K-means preprocessing is not solid and the background noise contributes to the misclassification results.


\item Looking at the confusion matrix, we believe the main causes for misclassification
      \begin{itemize}
        \item [--] Very fine differences between species, which is hard even for human experts
        \item [--] Noisy and possibly non-representative train data lead to overfitting,
      \end{itemize}
\end{enumerate}

\section{Conclusion and Future work}
  \begin{enumerate}
    \item Acquire more data and fine-tune ConvNet to solve overfitting problem
    \item Engage advanced techniques for image augmentation
    \item Explore state-of-art method to detect and locate leaf for Image CLEF natural leaf dataset.
  \end{enumerate}


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices

% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

  \bibitem{Cho2002}
    S. Cho, D. Lee, and J. Jeong. Automation and emerging technologies: Weed–plant discrimination by machine vision and artificial neural network. Biosystems Engineering, 83(3):275–280, 2002.

  \bibitem{Charles13}
    Charles Mallah, James Cope, James Orwell. Plant Leaf Classification Using Probabilistic Integration of Shape, Texture and Margin Features. Signal Processing, Pattern Recognition and Applications, in press. 2013

  \bibitem{Pedro13}
    Pedro F. B. Silva, Andre R.S. Marcal, Rubim M. Almeida da Silva. Evaluation of Features for Leaf Discrimination. 2013. Springer Lecture Notes in Computer Science, Vol. 7950, 197-204.

  \bibitem{Itheri}
    Itheri Yahiaoui, Nicolas Herve, and Nozha Boujemaa. Shape-based image retrieval in botanical collections, Lecture Notes in Computer Science including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinfor matics, vol. 4261 LNCS, pp 357-364, 2006.

  \bibitem{ConvNetsLeaf}
    Jassmann TJ, Tashakkori R, Parry RM (2015) Leaf classification utilizing a convolutional neural network. In: SoutheastCon

  \bibitem{Neeraj}
    Neeraj Kumar, Peter N Belhumeur, Arijit Biswas, David W Jacobs, W John Kress, Ida C Lopez, and João VB Soares, “Leafsnap: A computer vision system for automatic plant species identification,” in ECCV, pp. 502–516. Springer, 2012.

  \bibitem{David2015}
    DavidHall,ChrisMcCool,FerasDayoub,NikoSunderhauf, and Ben Upcroft, “Evaluation of features for leaf classification in challenging conditions,” 2015.

  \bibitem{Monica2014}
    Mónica G Larese, Ariel E Bayá, Roque M Craviotto, Miriam R Arango, Carina Gallo, and Pablo M Granitto, “Multiscale recognition of legume varieties based on leaf venation images,” Expert Systems with Applications, vol. 41, no. 10, pp. 4638–4647, 2014.

  \bibitem{Hasim2016}
    Hasim A, Herdiyeni Y, Douady S (2016) Leaf shape recognition using centroid contour distance. In: IOP conference series: earth and environmental science, p 012002

  \bibitem{Hall2015}
    Hall, David, McCool, Chris, Dayoub, Feras, Sunderhauf, Niko, \& Upcroft, Ben (2015), Evaluation of features for leaf classification in challenging conditions. In IEEE Winter Conference on Applications of Computer Vision (WACV 2015), 6-9 January 2015, Big Island, Hawaii, USA.


 \bibitem{Alex2014}
    A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012.

  \bibitem{Caffe}
    Y. Jia. Caffe: An open source convolutional architecture for fast feature embedding. http://caffe.berkeleyvision.org, 2013.

  \bibitem{ILSVRC2014}
    O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge. arXiv preprint arXiv:1409.0575, 2014.

  \bibitem{UCIDataSet}
    https://archive.ics.uci.edu/ml/datasets/Leaf

  \bibitem{SwedishLeafDataset}
    Oskar J. O. Söderkvist. Computer vision classifcation of leaves from swedish trees. Master's Thesis, Linkoping University, 2001.

  \bibitem{FlaviaDataset}
    Stephen Gang Wu, Forrest Sheng Bao, Eric You Xu, Yu-Xuan Wang, Yi-Fan Chang and Chiao-Liang Shiang, A Leaf Recognition Algorithm for Plant classification Using Probabilistic Neural Network, IEEE 7th International Symposium on Signal Processing and Information Technology, Dec. 2007, Cario, Egypt

  \bibitem{ImageCLEF2013}
    http://www.imageclef.org/2013/plant

  \bibitem{Keras}
    Chollet, Fran\c{c}ois, 2015, \url{https://github.com/fchollet/keras},

  \bibitem{KerasBlog}
    Chollet, Fran\c{c}ois, 2015, \url{ https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}

  \bibitem{CS231N}
    Andrej Karpathy et al. \url{http://cs231n.github.io/transfer-learning/}

  \bibitem{ResNet}
    Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition 	arXiv:1512.03385, Dec. 2015.

  \bibitem{VGGNetReference}
  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni- tion. arXiv preprint arXiv:1409.1556, 2014.

  \bibitem{GoogleNetReference}
  Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.

  \bibitem{OffTheShelf}
    Ali Sharif Razavian Hossein Azizpour Josephine Sullivan Stefan Carlsson. CNN Features off-the-shelf: an Astounding Baseline for Recognition, CVAP, KTH (Royal Institute of Technology) Stockholm, Sweden, May. 2014.

  \bibitem{Jason2014}
    Jason Yosinski,1 Jeff Clune,2 Yoshua Bengio,3 and Hod Lipson4. How transferable are features in deep neural networks? arXiv:1411.1792, Nov. 2014.

  \bibitem{Deep2013}
    David Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architectures using a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013.

  \bibitem{Dropout}
    N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. J. Machine Learning Res. 15, 1929–1958 (2014)

  \bibitem{Grabcut}
    Rother, Carsten, Vladimir Kolmogorov, and Andrew Blake. "Grabcut: Interactive foreground extraction using iterated graph cuts." ACM transactions on graphics (TOG). Vol. 23. No. 3. ACM, 2004.
  \bibitem{CLAHE}
    Reza, Ali M. "Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement." Journal of VLSI signal processing systems for signal, image and video technology 38.1 (2004): 35-44.
  \bibitem{deconv}
    Idier, Jérôme, ed. Bayesian approach to inverse problems. John Wiley \& Sons, 2013.

  \bibitem{SIFT}
    Lindeberg, Tony. "Scale invariant feature transform." Scholarpedia 7.5 (2012): 10491.

  \bibitem{HOG}
    Dalal, Navneet, and Bill Triggs. "Histograms of oriented gradients for human detection." 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). Vol. 1. IEEE, 2005.

    \bibitem{EvaluationOfLeafConv}
  Hall, David, et al. "Evaluation of features for leaf classification in challenging conditions." 2015 IEEE Winter Conference on Applications of Computer Vision. IEEE, 2015.
    \bibitem{PlantIdentificationConv}
  Lee, Sue Han, et al. "Plant identification system based on a convolutional neural network for the lifeclef 2016 plant classification task." Working notes of CLEF 2016 conference. 2016.

    \bibitem{CNNOfTheShelf}
  Sharif Razavian, Ali, et al. "CNN features off-the-shelf: an astounding baseline for recognition." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2014.

  \bibitem{tSNE}
  Maaten, Laurens van der, and Geoffrey Hinton. "Visualizing data using t-SNE." Journal of Machine Learning Research 9.Nov (2008): 2579-2605.

  \bibitem{DeepVizTool}
  Yosinski, Jason, et al. "Understanding neural networks through deep visualization." arXiv preprint arXiv:1506.06579 (2015).

\end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}
